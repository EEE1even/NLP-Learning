# IS-CSE

[Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding](https://arxiv.org/pdf/2305.07424.pdf)

**以前的方法怎么做？**

* 在无监督对比学习中，一些工作通过优化选择负样本例；使用预设prompt来提取特征；研究扩增对构建句子的影响

* 每个用于对比学习的embedding只来自一个句子实例，每个embedding被视为自己的一个独特类
* 正样本例都是通过数据扩增获得，负样本例从随机的句子集合中抽取（SimCSE）
* 通过词语重复、插入、删除来对输入句子进行扩增（ESimCSE）
* 随机翻转单词中的第一个字母来对输入进行扩增（CARDS）

​	

**以前的方法有什么问题？**

* 影响泛化能力

​	几乎上述的所有方法都将每个句子作为一个独特的类，在一个批次中将其与其他句子分开。这会导致模型“自信”的认为每个句子都是一个独立的类，因为在无监督学习中会有一些 false negative pairs 

* 从学习到的高斯分布中抽样生成负样本例，并过滤掉相似度高的负样本例（DCLR），但却并没有利用丰富的正样本emb

没有考虑通过采样的语义相似句子的emb来平滑正样本对



**现在的方法怎么做？**

受到label smoothing启发

* 从 dynamic memory buffer 中根据语义相似度来获取正样本组

* 在组中的emb通过自注意力机制进行汇总产生平滑的实例emb，防止模型的“自信”行为

* 通过对检索到的语义相似emb进行加权平均操作，来平滑标签

* 使用先进先出的内存缓冲，在训练的前几个步骤中将sent-emb存储，在构建正例对的时候根据余弦相似度取回，然后通过加权平均操作使得正样本emb平滑

* 同时使用样例和平滑样例来进行辨别

  

**现在的方法有什么优点？**

* 可以促使每个句子与其附近的其他句子相似，而不仅仅是其本身

* 提高了alignment，但是uniformity上有所下降

* 减少计算消耗

* 构造了平滑的emb

  
# Loss Function

## Cross-Entropy Loss

[知乎](https://zhuanlan.zhihu.com/p/638725320)

### 熵

$-\sum概率\times实际信息值=信息量期望值=熵$ 

实际信息量会使用 $-log$​ 来表示
$$
H(P) = -\sum_{i=1}^{n} P(X = x_i) \log_2 P(X = x_i)
$$

### 交叉熵

$$
H(P, Q) = -\sum_{i=1}^{n} P(X = x_i) \log Q(X = x_i)
$$

### 二分类

$$
L(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
$$

### 多分类

$$
L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y_i})
$$

### KL散度

$$
D_{KL} (P \| Q) =H(P,Q)-H(P)
$$


$$
=-\sum_{i=1}^{n} P(x_i) \log Q (x_i) - \left(-\sum_{i=1}^{n} P(x_i) \log P (x_i)\right) = \sum_{i=1}^{n} P (x_i) \log \frac{P(x_i)}{Q (x_i)}
$$


### 为什么使用交叉熵?

​	训练目的是使得网络实际输出Q尽量逼近真实分布P，由于P为真实分布所以熵H(P)为固定值，故KL散度的大小只由交叉熵决定，所以最小化KL散度等于最小化交叉熵


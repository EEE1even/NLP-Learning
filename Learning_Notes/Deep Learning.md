# Deep Learning

## 1、黑盒内部

**理解神经网络和深度学习：**

本质上来说神经网络可以拟合任何东西，不管这个数据集有多抽象，神经网络都能拟合出一条关于这个数据集的函数。

深度学习就是因为神经网络的层数很深，所以叫深度学习

神经网络 = big fancy squiggle fitting machines



**激活函数的意义：**

不同Dosage的指标经过激活函数后再相加，所得结果就是绿色曲线上的点



## 链式法则

**Intercept（截距）**:

- 在线性回归中，截距是指当所有自变量（解释变量）的值为零时，因变量的期望值。在 $y = mx + b$ 形式的线性方程中， $b$ 就是截距，它是直线与 $y$ 轴的交点的 $y$-坐标。
- 截距是模型的一个参数，它表示了数据在因变量轴上的起始点。

**Bias（偏差）**:

- 在统计学中，偏差是指一个估计量与被估计的真实值之间的差异。如果一个估计量系统地偏离真实值，那么它就是有偏的。
- 在机器学习中，偏差是指算法在训练数据上的表现与在未知数据上的表现之间的差距。高偏差通常意味着模型过于简单（即欠拟合），无法捕捉数据中的模式。

虽然在某些上下文中，“bias”可以用来描述类似于截距的常数项（如在神经网络的权重中），但在统计和回归分析中，它们通常指的是不同的概念。截距是模型的一个直接参数，而偏差是对模型估计精度的一种评估。



 A关于B，B关于C，通过链式法则可以知道A关于C

链式求导



### 梯度下降



在离最优解很远时跨度很大，在靠近最优解时步长减小

对损失函数求导，当导数等于0时说明到了最小值

学习率就是调整梯度下降步长的做法，$原先所在位置-（所在位置的导数值\times学习率）$ ，所得结果就是需要走到的位置

* 导数值越大，所在点的左右两侧越陡峭，意味着所走步长越大
* 导数值越小，所在点趋近函数的最小值，意味着要缩小步长

> 当有多个需要调整的参数时，这里的导数就是Loss函数对该参数的偏导数值

**步骤**

1、对损失函数中的每一个参数求导，用行话来说，就是对损失函数求梯度

2、随机给参数赋值

3、将参数值传到导数中（梯度）
4、计算步长： $Step\ Size=Slop\times Learning\ Rate$

5、计算新的参数值： $New\ Parameter=Old\ Parameter - Step\ Size$

3到5的步骤循环操作直到步长非常小或者达到最大步数



## 反向传播

standard deviation 标准差

standard normal distribution 标准正态分布



反向传播所计算的偏导数公式不会因为优化参数而改变，公式还是公式，改变的只是参数

softplus激活函数： $log(1+e^x)$ 

 

**链式法则 + 梯度下降** 是反向传播的核心



## ArgMax and Softmax



**argmax：** 最大值为1，其他都为0

**缺点：** 无法用它来优化神经网络中的权重和偏置，因为斜率为0，所以导数为0，无法进行梯度下降



**softmax：** $softmax_i(Output\ Values)=\frac{e^{Output\ Value_i}}{\sum^k_{j=1}e^{Output\ Value_i}}$ 



## 交叉熵

$Cross\ Entropy_p=-log_e(p)$ 

交叉熵可以放大预测错误的损失，导数值更大，反向传播后的步长跨度更大，更快趋向于Loss的最小值 










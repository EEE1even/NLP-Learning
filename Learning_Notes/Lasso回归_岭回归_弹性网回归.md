#  Lasso回归（L1）、 岭回归（L2）、弹性网回归

偏差（bais）与方差（variance）的区别

机器学习方法无法捕捉真实关系的现象被称为偏差

一般来说，训练集中偏差较小的测试集中方差可能很大



交叉验证（cross-validation）

最小二乘法（least square）

逻辑回归

knn是一个有监督方法

* 这里的k是指选择k个临近样本来进行投票，获得哪个族群的投票最多，就把这个点分类到该族群中

k-means是无监督方法

* 这里的K是你选择要将数据分成K个族群
* k随机选择，在elbow point的k是最佳簇数量

----

## Lasso回归：

罚函数为 $\lambda \times （|slope|+ |w_{1}|+|w_2|...）$ 

> 公式的括号中包含所有的权重变量，斜率和 $w$ 都算是一个等式的变量 

Lasso回归可以除去等式中的无用变量

相较于岭回归来说，Lasso回归对于包含许多无用变量的等式来说更加有用

在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选

## 岭回归：

岭回归在最小二乘法的基础上，在损失函数中加入了罚函数 $\lambda \times （slope^{2}+w_1^{2}+w_2^{2}...）$  ,slope是斜率, $\lambda$ 的存在是为了防止过拟合，值越大，预测值对斜率的敏感程度越低；尝试不同的 $\lambda$ 值来进行交叉验证，选择方差最小的值

> 公式的括号中包含所有的权重变量，斜率和 $w$ 都算是一个等式的变量 

允许拟合函数在训练集上有少许偏差，但是能降低其在测试集上的方差

最小二乘法算出的权重比岭回归算出的权重更加敏感（更加大）

在数据量比参数量少的情况下，岭回归可以解决最小二乘法解决不了的问题

## 弹性网回归（Elastic Net Regression）

罚函数 $\lambda_1 \times （|slope|+ |w_{1}|+|w_2|...）+\lambda_2 \times （slope^{2}+w_1^{2}+w_2^{2}...）$ 

将套索回归和岭回归的罚函数结合

> 公式的括号中包含所有的权重变量，斜率和 $w$ 都算是一个等式的变量 
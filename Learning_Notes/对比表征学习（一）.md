# 对比表征学习（一）

> 参考翁莉莲的[Blog](https://lilianweng.github.io/posts/2021-05-31-contrastive/#contrastive-loss)，本文主要聚焦于对比损失函数

对比表示学习（Contrastive Representation Learning）可以用来优化嵌入空间，使相似的数据靠近，不相似的数据拉远。同时在面对无监督数据集时，对比学习是一种极其有效的自监督学习方式

## 对比学习目标

在最早期的对比学习中只有一个正样本和一个负样本进行比较，在当前的训练目标中，一个批次的数据集中可以有多个正样本和负样本对。

### 对比损失函数

#### Contrastive loss 

该[论文](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf)是以对比方式进行深度度量学习（deep metric learning）的最早训练目标之一

给定一组输入样本 $$\{x_i\}$$，每个样本都有一个对应的标签 $$y_i \in \{1, \dots, L\}$$，共有 $$L$$ 个类别。我们希望学习一个函数 $$f_{\theta}(\cdot) : \mathcal{X} \rightarrow \mathbb{R}^d$$，该函数能将 $$x_i$$ 编码成一个嵌入向量，使得同一类别的样本具有相似的嵌入，而不同类别的样本具有非常不同的嵌入。因此，对比损失（Contrastive Loss）会取一对输入 $$(x_i, x_j)$$，并最小化同一类别样本间的嵌入距离，同时最大化不同类别样本间的嵌入距离。
$$
\mathcal{L}_{\text{cont}}(x_i, x_j, \theta) = \mathbf{1}[y_i = y_j] \left\| f_\theta(x_i) - f_\theta(x_j) \right\|^2 + \mathbf{1}[y_i \neq y_j] \max(0, \epsilon - \left\| f_\theta(x_i) - f_\theta(x_j) \right\|^2)
$$
其中 $$\epsilon$$​ 是一个超参数，用来定义不同类别样本的最低下界。

#### Triplet loss

参考[论文](https://arxiv.org/abs/1503.03832)，提出的目的是用来学习在不同姿势和角度下对同一个人进行人脸识别。

![image-20240527125804754](.\src\triplet.png)

给定一个锚定输入 $x$，我们选择一个正样本 $x^+$ 和一个负样本 $x^-$，意味着 $x^+$ 和 $x$ 属于同一类，而 $x^-$ 则来自另一个不同的类。三元组损失（Triplet Loss）通过以下公式学习，同时最小化锚定 $x$ 和正样本 $x^+$ 之间的距离，并最大化锚定 $x$ 和负样本 $x^-$​ 之间的距离：
$$
\mathcal{L}_{\text{triplet}}(x, x^+, x^-) = \sum_{x \in \mathcal{X}} \max \left(0, \|f(x) - f(x^+)\|^2 - \|f(x) - f(x^-)\|^2 + \epsilon \right)
$$
其中，边界参数 $\epsilon$ 被配置为相似对与不相似对之间距离的最小偏移量。



#### Lifted Structured Loss

参考[论文](https://arxiv.org/abs/1511.06452)，利用一个训练批次中的所有成对边缘，以提高计算效率。

![image-20240527130148375](.\src\LSL.png)

设 $D_{ij} = |f(x_i) - f(x_j)|_2 $，一个结构化的损失函数定义如下：
$$
\mathcal{L}_{\text{struct}} = \frac{1}{2|\mathcal{P}|} \sum_{(i,j) \in P} \max(0, \mathcal{L}_{\text{struct}}^{(ij)})^2
$$
其中，
$$
\mathcal{L}_{\text{struct}}^{(ij)} = D_{ij} + \max \left(\max_{(i,k) \in \mathcal{N}} (\epsilon - D_{ik}), \max_{(j,l) \in \mathcal{N}} (\epsilon - D_{jl})\right)
$$
这里 $\mathcal{P}$ 包含了正样本对的集合，而 $\mathcal{N}$ 是负样本对的集合。注意，密集的成对平方距离矩阵可以在每个训练批次中轻松计算。$\max \left(\max_{(i,k) \in \mathcal{N}} (\epsilon - D_{ik}), \max_{(j,l) \in \mathcal{N}} (\epsilon - D_{jl})\right)$ 部分用来挖掘难负样本，然而，这部分不是平滑的，可能会导致在实践中收敛到不好的局部最优。因此，它被放宽为以下形式：
$$
\mathcal{L}_{\text{struct}}^{(ij)} = D_{ij} + \log \left( \sum_{(i,k) \in \mathcal{N}} \exp(\epsilon-D_{ik}) + \sum_{(j,l) \in \mathcal{N}} \exp(\epsilon-D_{jl}) \right)
$$
在论文中，他们还提出通过在每个批次中积极加入一些难分的负样本(hard negative)，通过给出几对随机的正样本，来提高负样本的质量



#### N-pair loss

多类 N 对损失（[paper](https://papers.nips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html)）对三重损失进行了泛化，以包括与多个负样本的比较。

给定一个包含一个正样本和 $N-1$ 个负样本的 $N+1$ 元组（还要包括样本本身，所以N+1），训练样本为${\{x, x^+, {x_1}^-, \dots, {x_{N-1}}}^-\}$，损失被定义为：

$$
\mathcal{L}_{N\text{-pair}}(x, x^+, \{x_i\}_{i=1}^{N-1}) = \log \left( 1 + \sum_{i=1}^{N-1} \exp(f(x)^T f({x_i}^-) - f(x)^T f(x^+)) \right) \\ = -\log \frac{\exp(f(x)^T f(x^+))}{\exp(f(x)^T f(x^+)) + \sum_{i=1}^{N-1} \exp(f(x)^T f({x_i}^-)}
$$

如果我们每个类别只采样一个负样本，这等同于用于多分类的softmax损失。



#### NCE

**Noise Contrastive Estimation**，论文[链接](https://proceedings.mlr.press/v9/gutmann10a.html)

创新点是运行逻辑回归来区分目标数据和噪声。

让 $x$ 是目标样本，符合分布 $P(x|C = 1; \theta) = p_\theta(x)$，并且 $\tilde{x}$ 是噪声样本，符合分布 $P(\tilde{x}|C = 0) = q(\tilde{x})$。需要注意逻辑回归模型是模拟对数几率（即 logit），在这种情况下，我们希望对一个来自目标数据分布而非噪声分布的样本 $u$ 的 logit 进行建模：

$$
\ell(u) = \log \frac{p_\theta(u)}{q(u)} = \log p_\theta(u) - \log q(u)
$$

将 logits 转换成概率后，通过 sigmoid 函数 $\sigma(\cdot)$，我们可以应用交叉熵损失：

$$
L_{NCE} = -\frac{1}{N} \sum_{i=1}^N \left[ \log \sigma(\ell(x_i)) + \log (1 - \sigma(\ell(\tilde{x}_i))) \right]
$$

其中：

$$
\sigma(\ell) = \frac{1}{1 + \exp(-\ell)} = \frac{p_\theta}{p_\theta + q}
$$

这里列出了NCE损失的原始形式，它仅使用了一个正样本和一个噪声样本。在许多后续工作中，融合多个负样本的对比损失也广泛被称为NCE。



#### InfoNCE

论文[链接](https://arxiv.org/abs/1807.03748)，受到NCE的启发，InfoNCE使用分类交叉熵损失函数在一组不相关的噪声样本中寻找正例

给定一个上下文向量 $c$，正样本应该从条件分布 $p(x|c)$ 中抽取，而 $N-1$ 个负样本则从与上下文 $c$ 独立的提议分布 $p(x)$ 中抽取。为了简洁，让我们将所有样本标记为 $X = \{x_i\}_{i=1}^N$，其中只有一个 $x_{\text{pos}}$ 是正样本。我们正确检测到正样本的概率为：

$$
p(C = \text{pos} | X, c) = \frac{p(x_{\text{pos}}|c) \prod_{i=1, \dots, N; i \neq \text{pos}} p(x_i)}{\sum_{j=1}^N \left[ p(x_j|c) \prod_{i=1, \dots, N; i \neq j} p(x_i) \right]} =\frac{\frac{p(x_{\text{pos}} | c)}{p(x_{\text{pos}})}}{\sum_{j=1}^N \frac{p(x_j | c)}{p(x_j)}}= \frac{f(x_{\text{pos}}, c)}{\sum_{j=1}^N f(x_j, c)}
$$

其中，得分函数 $f(x, c) \propto \frac{p(x|c)}{p(x)}$。

InfoNCE 损失函数优化了正确分类正样本的负对数概率：

$$
\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}\left[\log \frac{f(x, c)}{\sum_{x'\in X} f(x', c)}\right]
$$

事实上，$f(x, c)$ 估计的密度比 $\frac{p(x|c)}{p(x)}$ 与互通信息优化有关。为了最大化输入 $x$ 和上下文向量 $c$ 之间的互通信息，我们有：

$$
I(x; c) = \sum_{x,c} p(x, c) \log \frac{p(x|c)}{p(x)p(c)} = \sum_{x,c} p(x, c) \log \frac{p(x|c)}{p(x)}
$$

其中，$\log \frac{p(x|c)}{p(x)}$ 的对数项由 $f$ 估计。

对于序列预测任务，CPC（Contrastive Predictive Coding）模型并不直接建模未来的观测 $p_k(X_{t+k} | C_t)$（这可能相当昂贵），而是模型一个密度函数以保留 $X_{t+k}$ 和 $C_t$ 之间的互信息：

$$
f_k(X_{t+k}, c_t) = \exp(z_{t+k}^T W_k c_t) \propto \frac{p(X_{t+k}|c_t)}{p(X_{t+k})}
$$

其中 $z_{t+k}$ 是编码后的输入，$W_k$ 是一个可训练的权重矩阵。



#### Soft-Nearest Neighbors Loss

Soft-Nearest Neighbors Loss被[Salakhutdinov & Hinton (2007)](https://proceedings.mlr.press/v2/salakhutdinov07a.html) 和[Frosst等人（2019）](https://arxiv.org/abs/1902.01889)进一步扩展，以包含多个正样本。

给定一批样本 $\{x_i, y_i\}_{i=1}^B$，其中 $y_i$ 是 $x_i$ 的类别标签，以及一个用于测量两个输入相似性的函数 $f(\cdot, \cdot)$，在温度 $\tau$ 下定义的软最近邻损失为：

$$
\mathcal{L}_{snn} = -\frac{1}{B} \sum_{i=1}^B \log \frac{\sum_{j \neq i, y_j = y_i} \exp(-\frac{f(x_i, x_j)}{\tau})}{\sum_{k \neq i} \exp(-\frac{f(x_i, x_k)}{\tau})}
$$

温度参数 $\tau$ 用于调整特征在表示空间中的集中程度。例如，当温度较低时，损失主要由小距离主导，而广泛分散的表征无法产生很大的贡献，因此变得无关紧要。

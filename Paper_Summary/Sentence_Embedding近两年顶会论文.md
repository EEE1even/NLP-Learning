## EMNLP 2023

**[AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification](https://aclanthology.org/2023.emnlp-main.208/)**

**[Contrastive Learning of Sentence Embeddings from Scratch](https://aclanthology.org/2023.emnlp-main.238/)**

**[Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings](https://aclanthology.org/2023.emnlp-main.359/)**

**[OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding](https://aclanthology.org/2023.emnlp-main.448/)**

**[SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives](https://aclanthology.org/2023.emnlp-main.737/)**

**[Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations](https://aclanthology.org/2023.emnlp-main.900/)**

**[Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs](https://aclanthology.org/2023.emnlp-main.1008/)**

**[SentAlign: Accurate and Scalable Sentence Alignment](https://aclanthology.org/2023.emnlp-demo.22/)**

**[Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization](https://aclanthology.org/2023.emnlp-industry.25/)**

**[HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings](https://aclanthology.org/2023.findings-emnlp.161/)**

**[RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training](https://aclanthology.org/2023.findings-emnlp.305/)**

**[Improving Contrastive Learning of Sentence Embeddings with Focal InfoNCE](https://aclanthology.org/2023.findings-emnlp.315/)**

**[DistillCSE: Distilled Contrastive Learning for Sentence Embeddings](https://aclanthology.org/2023.findings-emnlp.547/)**

**[Role of Context in Unsupervised Sentence Representation Learning: the Case of Dialog Act Modeling](https://aclanthology.org/2023.findings-emnlp.588/)**

**[On the Dimensionality of Sentence Embeddings](https://aclanthology.org/2023.findings-emnlp.694/)**

**[Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words](https://aclanthology.org/2023.findings-emnlp.729/)**

**[Improving Multi-Criteria Chinese Word Segmentation through Learning Sentence Representation](https://aclanthology.org/2023.findings-emnlp.850/)**

**[BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings](https://aclanthology.org/2023.findings-emnlp.1030/)**

**[Simplify: Automatic Arabic Sentence Simplification using Word Embeddings](https://aclanthology.org/2023.arabicnlp-1.35/)**

**[Investigating Semantic Subspaces of Transformer Sentence Embeddings through Linear Structural Probing](https://aclanthology.org/2023.blackboxnlp-1.11/)**

**[Contrastive Learning for Universal Zero-Shot NLI with Cross-Lingual Sentence Embeddings](https://aclanthology.org/2023.mrl-1.18/)**

**[Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models](https://aclanthology.org/2023.nlposs-1.2/)**

**[A Sentence Alignment Approach to Document Alignment and Multi-faceted Filtering for Curating Parallel Sentence Pairs from Web-crawled Data](https://aclanthology.org/2023.wmt-1.38/)**

## ACL 2023

**[Dual-Alignment Pre-training for Cross-lingual Sentence Embedding](https://aclanthology.org/2023.acl-long.191/)**

* [code](https://github.com/ChillingDream/DAP)

**[miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings](https://aclanthology.org/2023.acl-long.339/)**

**[WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings](https://aclanthology.org/2023.acl-long.677/)**

**[Composition-contrastive Learning for Sentence Embeddings](https://aclanthology.org/2023.acl-long.882/)**

**[Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity](https://aclanthology.org/2023.acl-short.49/)**

**[Improving Contrastive Learning of Sentence Embeddings from AI Feedback](https://aclanthology.org/2023.findings-acl.707/)**

**[Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence](https://aclanthology.org/2023.findings-acl.881/)**

**[Alleviating Over-smoothing for Unsupervised Sentence Representation](https://aclanthology.org/2023.acl-long.197/)**

**[RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank](https://aclanthology.org/2023.acl-long.771/)**

**[Ranking-Enhanced Unsupervised Sentence Representation Learning](https://aclanthology.org/2023.acl-long.879/)**

**[Self-Supervised Sentence Polishing by Adding Engaging Modifiers](https://aclanthology.org/2023.acl-demo.48/)**

**[Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity](https://aclanthology.org/2023.findings-acl.439/)**

**[Separating Context and Pattern: Learning Disentangled Sentence Representations for Low-Resource Extractive Summarization](https://aclanthology.org/2023.findings-acl.479/)**

**[Clustering-Aware Negative Sampling for Unsupervised Sentence Representation](https://aclanthology.org/2023.findings-acl.555/)**

**[On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning](https://aclanthology.org/2023.findings-acl.778/)**

## RepL4NLP 2023

**[Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model](https://aclanthology.org/2023.repl4nlp-1.26/)**

**[Relational Sentence Embedding for Flexible Semantic Matching](https://aclanthology.org/2023.repl4nlp-1.20/)**

**[Grammatical information in BERT sentence embeddings as two-dimensional arrays](https://aclanthology.org/2023.repl4nlp-1.3/)**



## EACL 2023

**[LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation](https://aclanthology.org/2023.eacl-main.138/)**

**[Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?](https://aclanthology.org/2023.findings-eacl.174/)**

**[Improving Contrastive Learning of Sentence Embeddings from AI Feedback](https://aclanthology.org/2023.findings-acl.707/)**

**[RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training](https://aclanthology.org/2023.findings-emnlp.305/)**

**[Improving Contrastive Learning of Sentence Embeddings with Focal InfoNCE](https://aclanthology.org/2023.findings-emnlp.315/)**

**[DistillCSE: Distilled Contrastive Learning for Sentence Embeddings](https://aclanthology.org/2023.findings-emnlp.547/)**



## EMNLP 2022

**[Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding](https://aclanthology.org/2022.emnlp-main.275/)**

**[Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation](https://aclanthology.org/2022.emnlp-main.433/)**

**[PromptBERT: Improving BERT Sentence Embeddings with Prompts](https://aclanthology.org/2022.emnlp-main.603/)**

**[English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings](https://aclanthology.org/2022.emnlp-main.621/)**

**[PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings](https://aclanthology.org/2022.emnlp-main.826/)**



## ACL 2022

**[Language-agnostic BERT Sentence Embedding](https://aclanthology.org/2022.acl-long.62/)**



## CoLING 2022

**[ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding](https://aclanthology.org/2022.coling-1.342/)**

**[An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning](https://aclanthology.org/2022.coling-1.426/)**

**[Smoothed Contrastive Learning for Unsupervised Sentence Embedding](https://aclanthology.org/2022.coling-1.434/)**

**[Incorporating the Rhetoric of Scientific Language into Sentence Embeddings using Phrase-guided Distant Supervision and Metric Learning](https://aclanthology.org/2022.sdp-1.7/)**